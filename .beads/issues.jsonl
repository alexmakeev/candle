{"id":"candle-16b-uys","title":"Qwen3-Omni BF16 on wgpu","description":"Full pipeline: wgpu integration testing, shader optimization, model loading","status":"open","priority":2,"issue_type":"epic","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:51:45.996593281Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:51:45.996593281Z"}
{"id":"candle-16b-uys.1","title":"Test wgpu_basics on Lyuda","description":"Run cargo run --example wgpu_basics --features wgpu on Radeon 8060S. Verify: adapter detection, tensor creation, matmul, BF16","status":"open","priority":1,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:51:53.709253682Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:51:53.709253682Z","dependencies":[{"issue_id":"candle-16b-uys.1","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:51:53.711486284Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.2","title":"Benchmark matmul performance on GPU vs CPU","description":"Compare 512x512 and 2048x2048 matmul times. GPU should be 50-100x faster than llvmpipe software","status":"open","priority":1,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.204724463Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.204724463Z","dependencies":[{"issue_id":"candle-16b-uys.2","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.207732821Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.2","depends_on_id":"candle-16b-uys.1","type":"blocks","created_at":"2026-01-28T16:52:42.819475212Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.3","title":"Implement GPU softmax shader","description":"WGSL shader for softmax. Currently falls back to CPU which is slow for BF16","status":"open","priority":2,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.261768554Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.261768554Z","dependencies":[{"issue_id":"candle-16b-uys.3","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.265163027Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.4","title":"Implement GPU layer_norm shader","description":"WGSL shader for layer normalization. Critical for transformer performance","status":"open","priority":2,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.323968566Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.323968566Z","dependencies":[{"issue_id":"candle-16b-uys.4","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.325285979Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.5","title":"Implement GPU RoPE shader","description":"Rotary Position Embedding on GPU. Essential for Qwen3 attention","status":"open","priority":2,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.383778069Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.383778069Z","dependencies":[{"issue_id":"candle-16b-uys.5","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.385306418Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.6","title":"Load Qwen3-Omni model on WgpuDevice","description":"Use VarBuilder::from_mmaped_safetensors with Device::Wgpu. Verify all 66GB loads to GPU VRAM","status":"open","priority":1,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.433381011Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.433381011Z","dependencies":[{"issue_id":"candle-16b-uys.6","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.434943996Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.6","depends_on_id":"candle-16b-uys.1","type":"blocks","created_at":"2026-01-28T16:52:42.867917826Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.6","depends_on_id":"candle-16b-uys.3","type":"blocks","created_at":"2026-01-28T16:52:42.915415495Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.6","depends_on_id":"candle-16b-uys.4","type":"blocks","created_at":"2026-01-28T16:52:42.959542787Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.6","depends_on_id":"candle-16b-uys.5","type":"blocks","created_at":"2026-01-28T16:52:43.033943744Z","created_by":"Alex Makeev"}]}
{"id":"candle-16b-uys.7","title":"Run text generation benchmark","description":"Generate 100 tokens with Qwen3-Omni on wgpu. Compare latency with CPU baseline","status":"open","priority":1,"issue_type":"task","owner":"alex@robobobr.ru","created_at":"2026-01-28T16:52:05.485962825Z","created_by":"Alex Makeev","updated_at":"2026-01-28T16:52:05.485962825Z","dependencies":[{"issue_id":"candle-16b-uys.7","depends_on_id":"candle-16b-uys","type":"parent-child","created_at":"2026-01-28T16:52:05.487667676Z","created_by":"Alex Makeev"},{"issue_id":"candle-16b-uys.7","depends_on_id":"candle-16b-uys.6","type":"blocks","created_at":"2026-01-28T16:52:43.078191182Z","created_by":"Alex Makeev"}]}
