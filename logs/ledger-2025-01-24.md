# Candle wgpu Backend — Ledger

## Project Goal
Build wgpu/Vulkan backend for Candle ML framework to run Qwen3-Omni on AMD Strix Halo APU.

**Hardware (Lyuda):**
- APU: AMD Ryzen AI Max+ 395 (Strix Halo)
- GPU: Radeon 8060S (RDNA 3.5, gfx1151)
- Memory: 128 GB unified (62 GB RAM / 64 GB VRAM via UMA Auto)

**Key Optimization:** Q8_0 quantization with DP4a (packed 4x8-bit integer dot product).

---

## 2025-01-24 18:00
Done: Completed Q8_0 quantization of Qwen3-Omni model
- Created streaming quantizer: `tensor-tools/src/streaming_quantize.rs`
- Processes tensors one-by-one to minimize memory usage
- Fixed dimension check: last_dim must be divisible by block_size (32)
- Original: 66 GB (BF16 SafeTensors) → Quantized: 36 GB (Q8_0 GGUF)
- Output: `/home/lluda/models/qwen3-omni-q8_0.gguf`

Next: Create `quantized_qwen3_omni` module for GGUF inference

## 2025-01-24 19:30
Done: Set up Beads task tracking for project
- Epic: candle-dlm — Qwen3-Omni Q8_0 quantization and inference
- Completed: candle-dlm.1 (build), .2 (estimate), .3 (quantize), .4 (verify)
- In progress: candle-dlm.5 — Create quantized_qwen3_omni module

Architecture decision:
- GGUF format required because SafeTensors doesn't support Q8_0 block format
- GGUF is only for our custom engine (llama.cpp doesn't support Qwen3-Omni architecture)
- Separate `quantized_qwen3_omni` module (not modifying existing BF16 version)
- Priority: TTS test — "Привет мир" → audio via Code2Wav component

Next: Delegate module creation to subagent

## 2025-01-24 20:00
Done: Started subagent for quantized module creation
- Subagent ae5b555 working on `quantized_qwen3_omni` module
- Should follow patterns from `quantized_qwen3_moe.rs`
- Focus on Code2Wav (TTS) first

Patterns to use:
- QMatMul wraps QTensor for linear layers
- MoE experts kept as Arc<QTensor>, gate dequantized to F32
- RmsNorm, Embedding dequantized once at load time
- Loading via Gguf helper from quantized_var_builder

Next: Monitor subagent progress, review created code

## 2025-01-24 21:00
Done: Code2Wav module complete and tested
- Subagent ae5b555 created full `quantized_qwen3_omni` module
- **Code2Wav works!** 100 tokens → 192013 samples (~8 sec @ 24kHz)
- Key insight: GGUF dequantize already returns correct tensor format, no permute needed
- Test example: `candle-examples/examples/qwen3_omni_tts/main.rs`

Created files:
- `quantized_qwen3_omni/mod.rs` — main module
- `quantized_qwen3_omni/config.rs` — configurations
- `quantized_qwen3_omni/gguf_loader.rs` — GGUF loader helpers
- `quantized_qwen3_omni/code2wav.rs` — **FULL vocoder** (20KB, 230 tensors)
- `quantized_qwen3_omni/{audio_tower,thinker,talker}.rs` — placeholders

Code2Wav architecture:
```
Codec Tokens → Embedding → PreTransformer (8 layers, RoPE)
           → Upsample (2 ConvNeXt blocks, 2x each)
           → HiFi-GAN Decoder (4 blocks: 8x, 5x, 4x, 3x = 480x total)
           → Audio Waveform
```

Next: Implement Talker (MoE 128 experts) for codec token generation

## 2025-01-24 22:30
Done: Talker (code_predictor) module complete and tested
- Implemented full Talker in `quantized_qwen3_omni/talker.rs`
- **Talker works!** Forward pass generates codec tokens [batch, seq, 15 codebooks]
- Tested incremental generation with KV cache
- Test example: `candle-examples/examples/qwen3_omni_talker/main.rs`

Architecture (code_predictor, NOT MoE - differs from BF16):
```
Codec Tokens [B, S, 15] → Sum of 15 Embeddings [B, S, 1024]
                       → 5 Transformer Layers (GQA 16/8 heads, head_dim=128)
                       → 15 LM Heads → Next Codec Tokens [B, S, 15, 2048]
```

Key config from GGUF analysis:
- hidden_size: 1024
- num_layers: 5
- num_attention_heads: 16 (q_proj: [2048, 1024])
- num_key_value_heads: 8 (k/v_proj: [1024, 1024])
- head_dim: 128 (from q_norm weight)
- intermediate_size: 3072 (from gate_proj)
- num_codebooks: 15
- codebook_size: 2048

Next: Connect Thinker hidden states to Talker (needs adapter layer)

## 2025-01-24 22:45
Done: Thinker + AudioTower modules complete
- AudioTower: 32 Whisper-style layers, Conv2d stem, mel → embeddings
- Thinker: 48 MoE layers (128 experts, top-8 routing)
- GQA attention with QK norms
- Compilation clean, no warnings

AudioTower architecture:
```
Mel Spectrogram [B, 1, 128, T]
    → Conv2d Stem (3 layers, 480 channels)
    → conv_out [7680 → 1280]
    → 32 Whisper Encoder Layers
    → proj [1280 → 2048]
    → Audio Embeddings for Thinker
```

Thinker architecture:
```
Text/Audio Embeddings [B, S, 2048]
    → 48 Decoder Layers (MoE 128 experts, top-8)
    → GQA (32 heads / 4 KV heads)
    → lm_head [152064 vocab]
    → ThinkerOutput {text_logits, hidden_states}
```

All quantized_qwen3_omni components complete!

Next: TTS test "Привет мир" — integrate full pipeline

## 2025-01-25 02:45
Done: Added hidden_projection to Talker for Thinker integration
- Added `HiddenProjection` struct: 2-layer MLP with SiLU activation
  - fc1: [2048, 2048] Q8_0 + bias F32
  - fc2: [1024, 2048] Q8_0 + bias F32
- New methods: `forward_from_hidden()`, `forward_from_hidden_logits()`, `has_hidden_projection()`
- GGUF tensors loaded:
  - `talker.hidden_projection.linear_fc1.{weight,bias}`
  - `talker.hidden_projection.linear_fc2.{weight,bias}`
- Integration test passed:
  - `Thinker.forward_text_only()` → hidden_states [1, 2, 2048]
  - `Talker.forward_from_hidden()` → codec_tokens [1, 2, 15]
- Test example: `candle-examples/examples/qwen3_omni_thinker_talker/main.rs`

Full TTS pipeline now connected:
```
Text Tokens → Thinker (MoE) → hidden_states [B, S, 2048]
           → hidden_projection (2048 → 1024)
           → Talker code_predictor → codec_tokens [B, S, 15]
           → Code2Wav vocoder → audio waveform
```

Next: Full TTS test "Привет мир" with tokenizer and audio output

---

## Key Files

| File | Purpose |
|------|---------|
| `tensor-tools/src/streaming_quantize.rs` | Streaming quantizer for large models |
| `candle-transformers/src/models/qwen3_omni/` | Original BF16 module |
| `candle-transformers/src/models/quantized_qwen3_omni/` | **Q8_0 module** (NEW) |
| `candle-transformers/src/models/quantized_qwen3_omni/code2wav.rs` | **TTS vocoder** (COMPLETE) |
| `candle-transformers/src/models/quantized_qwen3_omni/talker.rs` | **Talker code_predictor** (COMPLETE) |
| `candle-transformers/src/models/quantized_qwen3_omni/thinker.rs` | **Thinker MoE** (COMPLETE) |
| `candle-transformers/src/models/quantized_qwen3_omni/audio_tower.rs` | **AudioTower Whisper** (COMPLETE) |
| `candle-examples/examples/qwen3_omni_tts/` | Code2Wav test example |
| `candle-examples/examples/qwen3_omni_talker/` | Talker test example |
| `candle-examples/examples/qwen3_omni_thinker_talker/` | **Thinker→Talker integration test** (NEW) |
| `candle-wgpu/` | wgpu backend (to be integrated) |
| `candle-wgpu/src/quantized.rs` | Q8_0 matmul with DP4a |

## Remote Machine (Lyuda)

```bash
sshpass -p '1q2w3e' ssh -p 2222 lluda@127.0.0.1
source ~/.cargo/env  # before cargo commands
```

Model paths:
- Original: `/home/lluda/models/Qwen3-Omni-30B-A3B-Instruct/*.safetensors`
- Quantized: `/home/lluda/models/qwen3-omni-q8_0.gguf`

## 2026-01-24 21:44
--- SESSION END ---
